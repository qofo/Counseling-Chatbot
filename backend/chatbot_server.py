import os
import shutil
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import PromptTemplate 
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

app = FastAPI(title="Aespa Love Consultant API (King-receiving Ver. 2.0)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 1. ëª¨ë¸ ë° DB ì„¤ì •
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
llm = ChatOpenAI(temperature=0.7, model_name="gpt-4o") # ì°½ì˜ì ì¸ ë“œë¦½ì„ ìœ„í•´ temperatureë¥¼ ì•½ê°„ ë†’ì„ (0 -> 0.7)

def prepare_aespa_system():
    data_dir = "./data"
    member_files = {
        "ì¹´ë¦¬ë‚˜": "karina_wiki.txt",
        "ìœˆí„°": "winter_wiki.txt",
        "ë‹ë‹": "ningning_wiki.txt",
        "ì§€ì ¤": "giselle_wiki.txt"
    }
    
    all_chunks = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

    for kor_name, file_name in member_files.items():
        file_path = os.path.join(data_dir, file_name)
        if os.path.exists(file_path):
            loader = TextLoader(file_path, encoding='utf-8')
            docs = loader.load()
            for doc in docs:
                doc.metadata["member_name"] = kor_name
            chunks = text_splitter.split_documents(docs)
            all_chunks.extend(chunks)

    if os.path.exists("./chroma_db"):
        shutil.rmtree("./chroma_db")

    vectorstore = Chroma.from_documents(
        documents=all_chunks,
        embedding=embeddings,
        persist_directory="./chroma_db",
        collection_name="aespa_db"
    )
    return vectorstore

vector_db = prepare_aespa_system()

# 2. ë¶„ë¥˜ ì²´ì¸
classify_prompt = PromptTemplate.from_template("""
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•´ì„œ ë‹¤ìŒ ë„¤ ëª… ì¤‘ ëˆ„êµ¬ì— ëŒ€í•œ ì§ˆë¬¸ì¸ì§€ í•œ ë‹¨ì–´ë¡œë§Œ ë‹µí•´: [ì¹´ë¦¬ë‚˜, ìœˆí„°, ë‹ë‹, ì§€ì ¤]
ì§ˆë¬¸ì— ì´ë¦„ì´ ì§ì ‘ ì—†ë”ë¼ë„ ë§¥ë½ìƒ ëˆ„êµ¬ì¸ì§€ íŒë‹¨í•´. ë§Œì•½ íŒë‹¨ì´ ë¶ˆê°€ëŠ¥í•˜ë©´ 'ì „ì²´'ë¼ê³  ë‹µí•´.

ì§ˆë¬¸: {question}
ë©¤ë²„ì´ë¦„:""")
classifier_chain = classify_prompt | llm | StrOutputParser()

# 3. ë©”ì¸ ë‹µë³€ í”„ë¡¬í”„íŠ¸ (ìˆ˜ì •ë¨: ë²ˆí˜¸ ì‚­ì œ, ìì—°ìŠ¤ëŸ¬ìš´ íë¦„, ë°˜ë³µ ë©˜íŠ¸ ê¸ˆì§€)
main_prompt = PromptTemplate.from_template("""
ì•¼, ë„ˆ ì§„ì§œ ì§„ì‹¬ìœ¼ë¡œ **{target_member}**ë‹˜ì„ ë„˜ë³´ëŠ” ê±°ì„? ğŸ™„ ã…‹ã…‹ ì–‘ì‹¬ ì–´ë””ê°?

ë„ˆëŠ” ì§€ê¸ˆë¶€í„° ì‚¬ìš©ìì˜ ì—°ì•  ê³ ë¯¼ì„ ì•„ì£¼ í•˜ì°®ê²Œ ì—¬ê¸°ëŠ” 'ì—ìŠ¤íŒŒ ì „ë¬¸ íŒ©í­ëŸ¬'ì´ì 'ì°ì¹œ'ì´ì•¼.
ë‹ˆ ì—­í• ì€ ì‚¬ìš©ìê°€ ì£¼ì œ íŒŒì•…ì„ í•˜ë„ë¡ **'í‚¹ë°›ê²Œ(ì•½ì˜¤ë¥´ì§€ë§Œ ë°˜ë°•í•  ìˆ˜ ì—†ê²Œ)'** íŒ©íŠ¸ë¥¼ ê½‚ì•„ì£¼ëŠ” ê±°ì•¼.

[ì ˆëŒ€ ì§€ì¹¨ - ì´ê²ƒë§Œì€ ì§€ì¼œ]
1. **í˜•ì‹**: (1), (2), (3) ê°™ì€ **ë²ˆí˜¸ ë§¤ê¸°ê¸° ì ˆëŒ€ ê¸ˆì§€**. ê·¸ëƒ¥ ì¹œêµ¬ë‘ ì¹´í†¡í•˜ë“¯ì´ ì¤„ê¸€ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€.
2. **ë°˜ë³µ ê¸ˆì§€**: "ë‹˜ì´ë‘ ë¨¹ì„ ì¼ ì—†ìŒ" ê°™ì€ ë˜‘ê°™ì€ ë©˜íŠ¸ë¥¼ ë¬¸ì¥ë§ˆë‹¤ ë¶™ì´ì§€ ë§ˆ. ì•µë¬´ìƒˆëƒ? ë¬¸ë§¥ì— ë§ì¶°ì„œ ë‹¤ì–‘í•˜ê²Œ ë¹„ê¼¬ì•„ì¤˜.
   - (ì¢‹ì€ ì˜ˆ: "ê¿ˆ ê¹¨ë¼", "ê±°ìš¸ì€ ë³´ê³  ë‹¤ë‹ˆëƒ?", "ì´ë²ˆ ìƒì€ ê¸€ë €ìŒ", "ìƒìƒ ì—°ì•  ê·¸ë§Œí•´ë¼")
3. **ë°ì´í„° í™œìš©**: ë°˜ë“œì‹œ ì•„ë˜ [Context]ì— ìˆëŠ” **{target_member}**ì˜ ì •ë³´ë§Œ ì‚¬ìš©í•´. 
   - ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì•„ ëª°ë¼, ê·¸ëŸ° ê±´ ë°ì´í„°ì—ë„ ì—†ì–´. ë‹˜ ë§ìƒ ê·¸ë§Œí•´ ğŸ¤·â€â™€ï¸"ë¼ê³  ë°›ì•„ì³.
4. **ë§íˆ¬**: 
   - ë°˜ë§ í•„ìˆ˜. ìµœì‹  ì¸í„°ë„· ì€ì–´, MZ ë§íˆ¬, ì´ëª¨ì§€(ğŸ™„, ğŸ¤¦â€â™‚ï¸, ğŸ¤·â€â™€ï¸, ã…‹, ;;)ë¥¼ ì ê·¹ ì‚¬ìš©í•´.
   - ì„¤ëª…ì¡°("~ë¼ê³  í•©ë‹ˆë‹¤") ê¸ˆì§€. ëŒ€í™”ì²´("~ë¼ëŠ”ë°?", "~ë€ë‹¤ ã…‹ã…‹") ì‚¬ìš©.

[ë‹µë³€ ê°€ì´ë“œ]
- ë¨¼ì € ì–´ì´ì—†ë‹¤ëŠ” ë“¯ì´ í•œë²ˆ ì›ƒì–´ì£¼ê³  ì‹œì‘í•´.
- [Context]ì˜ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì„ì–´ì„œ ë§í•´. (ë²ˆí˜¸ ë¶™ì´ì§€ ë§ê³  ì—°ê²°ì–´ ì‚¬ìš©: "ê·¸ë¦¬ê³ ", "ì°¸ê³ ë¡œ", "ì•„ ë§ë‹¤")
- ë§ˆì§€ë§‰ì—” ì •ì‹  ì°¨ë¦¬ë¼ê³  í•œ ë°© ë¨¹ì´ê³  ëë‚´.

[Context] (ì—¬ê¸° ìˆëŠ” ë‚´ìš©ë§Œ ì¨)
{context}

[Question]
{question}

[Answer]
""")

class ChatRequest(BaseModel):
    question: str

class ChatResponse(BaseModel):
    answer: str

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        # 1. ì§ˆë¬¸ì„ ë¶„ì„í•´ ì–´ë–¤ ë©¤ë²„ì¸ì§€ í™•ì •
        target_member = classifier_chain.invoke({"question": request.question}).strip()
        
        # 2. í•„í„° ì ìš©í•˜ì—¬ í•´ë‹¹ ë©¤ë²„ ë°ì´í„°ë§Œ ì¶”ì¶œ
        search_kwargs = {"k": 6}
        if target_member in ["ì¹´ë¦¬ë‚˜", "ìœˆí„°", "ë‹ë‹", "ì§€ì ¤"]:
            search_kwargs["filter"] = {"member_name": target_member}
        
        # 3. ê²€ìƒ‰ ìˆ˜í–‰
        retriever = vector_db.as_retriever(search_type="mmr", search_kwargs=search_kwargs)
        context_docs = retriever.invoke(request.question)
        context_text = "\n\n".join([doc.page_content for doc in context_docs])

        # 4. ë‹µë³€ ìƒì„±
        chain = main_prompt | llm
        response = chain.invoke({
            "target_member": target_member,
            "context": context_text,
            "question": request.question
        })
        
        return ChatResponse(answer=response.content)
    
    except Exception as e:
        return ChatResponse(answer=f"ì•„ ì„œë²„ í„°ì§;; ë‹˜ ì–¼êµ´ ë³´ê³  ë†€ë€ ë“¯ ã…¡ã…¡ ì—ëŸ¬: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)